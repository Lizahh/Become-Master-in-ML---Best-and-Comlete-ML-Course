{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Engineering and Data Praparation**\n",
        "\n",
        "* In the real world, almost 80% of time is spend on cleaning and formatting data.\n",
        "\n",
        "* Feature engineering is the process of creating new features or transforming existing features in a dataset 'to improve the performance of a machine learning model.' \n",
        "\n",
        "# **3 Approaches for Feature Engineering:**\n",
        "\n",
        "## **1. Extracting information from features:**\n",
        "\n",
        "  * It is most common way of feature engineering.\n",
        "  * Suppose we have dataset of where each row has a timestamp value:\n",
        "    \n",
        "    e.g. 1990-12-01 09:26:03\n",
        "  * In this current format, it cannot be passed directly to any ML model. **ML models can only work on float/int values.** \n",
        "  * So we can convert this feature to multiple features like:\n",
        "  Feature 1: Year: 1990, 1991 ... etc.\n",
        "  Feature 2: Month: 12, 11 ... etc.\n",
        "  etc.\n",
        "\n",
        "  * If we have some text data in some feature e.g.\n",
        "\n",
        "    'My name is Dr. Aliza Mustafa.'\n",
        "  \n",
        "  To extract different features from this text format as:\n",
        "  1. Length of text\n",
        "  2. Number of times a word is mentioned in text\n",
        "\n",
        "  Such handling of text is called Natural Language Processing (NLP)."
      ],
      "metadata": {
        "id": "ct5ZhuOl_mwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Combining Features:**\n",
        "\n",
        "* Sometimes combining features is more useful than using them individually. Recall when our model performed perfect when created interactions terms of them.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "waQG-8RXFEbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Transforming Informtion of Features:**\n",
        "\n",
        "* It is very common feature engineer approach for the string data in columns. \n",
        "* Often, categorical/ordinal features are presented as string data.\n",
        "* We can use 2 methods for transformating information of features with string data values:\n",
        "###  **1. Integer Encoding:**\n",
        "    * It directly converts categories into 1,2,3 .. etc numbers.\n",
        "    * Its one rule is: There must be some **'implied ordering'** relationship between all categories of that feature. Only then, we can perform integer encoding on them.\n",
        "    * \n",
        "    Implied Ordering: It means there should be some kind of ordering relationship in categories. E.g. if I have these categories:\n",
        "\n",
        "     **Spice Level**\n",
        "\n",
        "    Mild\n",
        "    \n",
        "    Hot\n",
        "    \n",
        "    Fire\n",
        "\n",
        "    We can directly say that there is ordering in these all. We know the lowest level of spice is mild, then hot is a bit higher and fire is highest level of spice.\n",
        "    \n",
        "    We can perform integer encoding on these as:\n",
        "    \n",
        "      **Spice Level**\n",
        "\n",
        "    Mild: 0\n",
        "    \n",
        "    Hot: 1\n",
        "    \n",
        "    Fire: 2\n",
        "\n",
        "    Mild is having 0, hot is having 1 etc showing the severity of spice that definitely makes sense. Fire definitely will be shown as 2. \n",
        "    \n",
        "    * Now if I have categories as:\n",
        "    \n",
        "    ***Countries:**\n",
        "\n",
        "    USA\n",
        "    \n",
        "    Canada\n",
        "\n",
        "    UK\n",
        "\n",
        "    What it shows?? How you can say USA is 0 and canada is 1 etc.?? There is no comparison of hierarchy in them. So it means the values in this feature doesn't have any implied ordering. \n",
        "\n",
        "    They are standalone values or labels representing distinct entities rather than being in a specific order or sequence.\n",
        "    So, we can't apply integer encoding on this feature. ðŸ˜“ \n",
        "\n",
        "    Now how to handle columns which do not have any implied ordering in them. We use second method for them which is called 'One-Hot Encoding'.\n",
        "\n",
        "     **Advantage:** Integer encoding is easier and doesn't increase number of features.\n",
        "    \n",
        "     **Disadvantage:** Its not always the case that always there is some ordered relationship between categories. So, it can't be used each time.\n",
        "\n",
        "  ###   **2. One Hot Encoding:**\n",
        "    * One-hot encoding doesn't care if theres some ordering in categories of feature.\n",
        "    * It just creates a dummy variable/feature for each category.\n",
        "    * Suppose it has this feature:\n",
        "\n",
        "      **Countries**\n",
        "        \n",
        "        USA\n",
        "      \n",
        "        UK\n",
        "\n",
        "        CANADA\n",
        "\n",
        "    * This 'countries' feature has 3 categories in it. So we can make 3 new features from it. And then remove that 'countries' feature. We can assign one to where its value exist i.e.\n",
        "\n",
        "    USA     UK CANADA\n",
        "    \n",
        "    1       0 0\n",
        "    \n",
        "    0       1  0\n",
        "     \n",
        "    0       0   1\n",
        "\n",
        "    * Tell me one thing. It I tell you that we have USA and UK only as:\n",
        "\n",
        "    USA     UK \n",
        "    \n",
        "    1       0 \n",
        "    \n",
        "    0       1  \n",
        "     \n",
        "    0       0   \n",
        "\n",
        "  * Now if I say I have canada too and ask where canada would be 1? You would directly say that on 3rd place. See!\n",
        "\n",
        "  * It means if we have all the features that we can easily judge that last one will be having 1 at the end, or first will be having 1 at the start. \n",
        "  \n",
        "  * So we can say that we can predict the value of one feature (CANADA) by using combination of other features (USA and UK). This is called **Multicollinearity.**\n",
        "  \n",
        "  * To handle this multicollinearity, we can do one favour to the ML model. delete that one exta feature (canada) (droplast = True). You can delete either USA (dropfirst = True) too as we can judge USA value with UK and canada. So, whatever category we will exclude will be called **reference category** and all the remaining will be called **binary categories**. This whole method of removing reference cateogry and keeping binary categories is called **dummy variable trap or reference encoding**. \n",
        "  * We can technically not losing any information with this. \n",
        "\n"
      ],
      "metadata": {
        "id": "ugJcrF2AGDi3"
      }
    }
  ]
}